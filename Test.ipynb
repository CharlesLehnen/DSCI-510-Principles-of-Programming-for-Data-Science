{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33801424",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2496637211.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [29], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    for i, box in enumerate(prediction[\"boxes\"]):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image, crop\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "\n",
    "# Folder containing images to process\n",
    "image_dir = \"data/images\"\n",
    "\n",
    "# Folder for cropped images\n",
    "cropped_dir = os.path.join(image_dir, \"cropped\")\n",
    "\n",
    "# Create a dictionary to map image filenames to their classification labels\n",
    "# This will be filled in with user-provided classification values\n",
    "image_labels = {}\n",
    "\n",
    "# Create the cropped directory if it doesn't exist\n",
    "if not os.path.exists(cropped_dir):\n",
    "    os.makedirs(cropped_dir)\n",
    "\n",
    "# Define a custom dataset class that wraps the cropped images and their labels\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform):\n",
    "        # Store the list of image filenames and labels\n",
    "        self.image_filenames = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir)]\n",
    "        self.image_labels = [image_labels[os.path.basename(filename)] for filename in self.image_filenames]\n",
    "\n",
    "        # Store the transforms object\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Read the image and its corresponding label\n",
    "        image = read_image(self.image_filenames[index])\n",
    "        label = self.image_labels[index]\n",
    "\n",
    "        # Apply the transform to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return the image and its label as a tuple\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def classify_and_crop(image_dir, cropped_dir):    \n",
    "    # Step 1: Initialize model with the best available weights\n",
    "    ## Change parameter here to adjust!!\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Process all images in the image directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        # Skip subdirectories\n",
    "        if os.path.isdir(os.path.join(image_dir, filename)):\n",
    "            continue\n",
    "\n",
    "        # Read the image\n",
    "        img = read_image(os.path.join(image_dir, filename))\n",
    "\n",
    "        # Apply inference preprocessing transforms\n",
    "        batch = [preprocess(img)]\n",
    "\n",
    "        # Use the model and visualize the prediction\n",
    "        prediction = model(batch)[0]\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                                  labels=labels,\n",
    "                                  colors=\"red\",\n",
    "                                  width=4, font_size=30)\n",
    "        im = to_pil_image(box.detach())\n",
    "\n",
    "        # Save the cropped image\n",
    "        im.save(os.path.join(cropped_dir, filename))\n",
    "\n",
    "\n",
    "\n",
    "        # Crop each image to the bounding box for each animal and save it to the cropped_dir directory\n",
    "        for i, box in enumerate(prediction[\"boxes\"]):\n",
    "            # Convert float values to integers\n",
    "            box = torch.round(box)\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "\n",
    "            # Compute the coordinates of the crop region based on the bounding box coordinates\n",
    "            top = y1\n",
    "            left = x1\n",
    "            height = y2 - y1\n",
    "            width = x2 - x1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_im = crop(im, top, left, height, width)\n",
    "\n",
    "            # Save the cropped image\n",
    "            cropped_im.save(os.path.join(cropped_dir, f\"{filename}_{i}.jpg\"))\n",
    "            \n",
    "            \n",
    "# Ask the user if they want to run the function\n",
    "should_run = input(\"Do you want to run the classify_and_crop function? (y/n)\")\n",
    "\n",
    "# Check the user's response and run the function if they want to\n",
    "if should_run.lower() == \"y\":\n",
    "  classify_and_crop(image_dir, cropped_dir)\n",
    "else:\n",
    "  print(\"The classify_and_crop function will not be run at this time.\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Split the paths of the cropped images into 70% training, 20% validation, and 10% test sets\n",
    "\n",
    "cropped_paths = [os.path.join(cropped_dir, filename) for filename in os.listdir(cropped_dir)]\n",
    "\n",
    "## Shuffle the paths\n",
    "random.shuffle(cropped_paths)\n",
    "\n",
    "## Split the paths\n",
    "train_paths = cropped_paths[:int(len(cropped_paths) * 0.7)]\n",
    "valid_paths = cropped_paths[int(len(cropped_paths) * 0.7):int(len(cropped_paths) * 0.9)]\n",
    "test_paths = cropped_paths[int(len(cropped_paths) * 0.9):]\n",
    "\n",
    "## For debugging\n",
    "## print(f'Training: {train_paths}')\n",
    "## print(f'Training: {valid_paths}')\n",
    "## print(f'Training: {test_paths}')\n",
    "\n",
    "\n",
    "# Manually re-classify images\n",
    "\n",
    "for path in train_paths + valid_paths:\n",
    "    # Open the image and display it to the user\n",
    "    img = Image.open(path)\n",
    "    img.show()\n",
    "\n",
    "    # Ask the user to classify the image\n",
    "    classification = input(\"Enter the classification for this image: \")\n",
    "\n",
    "    # Store the classification somewhere (e.g. in a database)\n",
    "    # ...\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9901d7b-19e9-461e-abca-f6d529aac3b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstart_time\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "# Process all images in the image directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Skip subdirectories\n",
    "    if os.path.isdir(os.path.join(image_dir, filename)):\n",
    "        continue\n",
    "\n",
    "    # Read the image and make predictions\n",
    "    img = read_image(os.path.join(image_dir, filename))\n",
    "    batch = [preprocess(img)]\n",
    "    prediction = model(batch)[0]\n",
    "\n",
    "    # Draw bounding boxes around each detected object\n",
    "    labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "    box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                              labels=labels,\n",
    "                              colors=\"red\",\n",
    "                              width=4, font_size=30)\n",
    "    im = to_pil_image(box.detach())\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    im.show()\n",
    "\n",
    "    # Ask the user to classify each detected object\n",
    "    for i, box in enumerate(prediction[\"boxes\"]):\n",
    "        label = input(f\"Enter a classification label for the object in box {i+1}: \")\n",
    "        image_labels[filename] = label\n",
    "\n",
    "    # Save the cropped image\n",
    "    im.save(os.path.join(cropped_dir, filename))\n",
    "\n",
    "# Create a custom AnimalDataset object\n",
    "transform = Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "dataset = AnimalDataset(image_dir, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e56d5953-153b-43c2-8075-af5ea6105370",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m             cropped_im\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cropped_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Ask the user if they want to run the function\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m should_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you want to run the classify_and_crop function? (y/n)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Check the user's response and run the function if they want to\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_run\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\dsci_510_final\\lib\\site-packages\\ipykernel\\kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\R-MINI~1\\envs\\dsci_510_final\\lib\\site-packages\\ipykernel\\kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image, crop\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Folder containing images to process\n",
    "image_dir = \"data/images\"\n",
    "\n",
    "# Folder for cropped images\n",
    "cropped_dir = os.path.join(image_dir, \"cropped\")\n",
    "\n",
    "# Create a dictionary to map image filenames to their classification labels\n",
    "# This will be filled in with user-provided classification values\n",
    "image_labels = {}\n",
    "\n",
    "\n",
    "# Create the cropped directory if it doesn't exist\n",
    "if not os.path.exists(cropped_dir):\n",
    "    os.makedirs(cropped_dir)\n",
    "    \n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform):\n",
    "        # Store the list of image filenames and labels\n",
    "        self.image_filenames = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir)]\n",
    "        self.image_labels = [image_labels[os.path.basename(filename)] for filename in self.image_filenames]\n",
    "\n",
    "        # Store the transforms object\n",
    "        self.transform = transform\n",
    "    \n",
    "    \n",
    "def classify_and_crop(image_dir, cropped_dir):\n",
    "    # Create a dictionary to map image filenames to their classification labels\n",
    "    image_labels = {}\n",
    "\n",
    "    # Step 1: Initialize model with the best available weights\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Process all images in the image directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        # Skip subdirectories\n",
    "        if os.path.isdir(os.path.join(image_dir, filename)):\n",
    "            continue\n",
    "\n",
    "        # Read the image and make predictions\n",
    "        img = read_image(os.path.join(image_dir, filename))\n",
    "        batch = [preprocess(img)]\n",
    "        prediction = model(batch)[0]\n",
    "\n",
    "        # Draw bounding boxes around each detected object\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                                  labels=labels,\n",
    "                                  colors=\"red\",\n",
    "                                  width=4, font_size=30)\n",
    "        im = to_pil_image(box.detach())\n",
    "\n",
    "        # Ask the user to classify each detected object\n",
    "        for i, box in enumerate(prediction[\"boxes\"]):\n",
    "            label = input(f\"Enter a classification label for the object in box {i+1}: \")\n",
    "            image_labels[filename] = label\n",
    "\n",
    "        # Save the cropped image\n",
    "        im.save(os.path.join(cropped_dir, filename))\n",
    "\n",
    "        # Crop each image to the bounding box for each animal and save it to the cropped_dir directory\n",
    "        for i, box in enumerate(prediction[\"boxes\"]):\n",
    "            # Convert float values to integers\n",
    "            box = torch.round(box)\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "\n",
    "            # Compute the coordinates of the crop region based on the bounding box coordinates\n",
    "            top = y1\n",
    "            left = x1\n",
    "            height = y2 - y1\n",
    "            width = x2 - x1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_im = crop(im, top, left, height, width)\n",
    "\n",
    "            # Save the cropped image\n",
    "            cropped_im.save(os.path.join(cropped_dir, f\"{filename}_{i}.jpg\"))\n",
    "            \n",
    "# Ask the user if they want to run the function\n",
    "should_run = input(\"Do you want to run the classify_and_crop function? (y/n)\")\n",
    "\n",
    "# Check the user's response and run the function if they want to\n",
    "if should_run.lower() == \"y\":\n",
    "  classify_and_crop(image_dir, cropped_dir)\n",
    "else:\n",
    "  print(\"The classify_and_crop function will not be run at this time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8fd22-0c0b-4c9f-b996-fbb1c80b948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform):\n",
    "        # Store the list of image filenames and labels\n",
    "        self.image_filenames = []\n",
    "        self.image_labels = []\n",
    "\n",
    "        # Store the transforms object\n",
    "        self.transform = transform\n",
    "    \n",
    "    def add_image(self, image, label):\n",
    "        # Add the image and label to the list of image filenames and labels\n",
    "        self.image_filenames.append(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78123dcd-570d-47bc-ba0b-8de405164969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "# ...\n",
    "\n",
    "# Process all images in the image directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Skip subdirectories\n",
    "    if os.path.isdir(os.path.join(image_dir, filename)):\n",
    "        continue\n",
    "\n",
    "    # Read the image and make predictions\n",
    "    img = read_image(os.path.join(image_dir, filename))\n",
    "    batch = [preprocess(img)]\n",
    "    prediction = model(batch)[0]\n",
    "\n",
    "    # Draw bounding boxes around each detected object\n",
    "    labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "    box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                              labels=labels,\n",
    "                              colors=\"red\",\n",
    "                              width=4, font_size=30)\n",
    "\n",
    "    # Convert the tensor to a PIL.Image object and display it\n",
    "    im = Image.fromarray(box.mul_(255).permute(1, 2, 0).byte().numpy())\n",
    "    imshow(im)\n",
    "\n",
    "    # Ask the user to classify each detected object\n",
    "    for i, box in enumerate(prediction[\"boxes\"]):\n",
    "        label = input(f\"Enter a classification label for the object in box {i+1}: \")\n",
    "        image_labels[filename] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a290f-c739-46dc-b8ca-aee2dcb6063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_crop(image_dir, cropped_dir):    \n",
    "    # Initialize the AnimalDataset object\n",
    "    dataset = AnimalDataset(transform)\n",
    "    \n",
    "    # Step 1: Initialize model with the best available weights\n",
    "    ## Change parameter here to adjust!!\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
    "    model.eval()\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Process all images in the image directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        # Skip subdirectories\n",
    "        if os.path.isdir(os.path.join(image_dir, filename)):\n",
    "            continue\n",
    "\n",
    "        # Read the image\n",
    "        img = read_image(os.path.join(image_dir, filename))\n",
    "\n",
    "        # Apply inference preprocessing transforms\n",
    "        batch = [preprocess(img)]\n",
    "\n",
    "        # Use the model and visualize the prediction\n",
    "        prediction = model(batch)[0]\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                                  labels=labels,\n",
    "                                  colors=\"red\",\n",
    "                                  width=4, font_size=30)\n",
    "        im = to_pil_image(box.detach())\n",
    "\n",
    "        # Save the cropped image\n",
    "        im.save(os.path.join(cropped_dir, filename))\n",
    "\n",
    "\n",
    "\n",
    "        # Crop each image to the bounding box for each animal and save it to the cropped_dir directory\n",
    "        for i, box in enumerate(prediction[\"boxes\"]):\n",
    "            # Convert float values to integers\n",
    "            box = torch.round(box)\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "\n",
    "            # Compute the coordinates of the crop region based on the bounding box coordinates\n",
    "            top = y1\n",
    "            left = x1\n",
    "            height = y2 - y1\n",
    "            width = x2 - x1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_im = crop(im, top, left, height, width)\n",
    "            \n",
    "            # Ask the user to classify the detected object\n",
    "            label = input(f\"Enter a classification label for the object in box {i+1}: \")\n",
    "            image_labels[filename] = label\n",
    "\n",
    "            # Add the image and label to the dataset\n",
    "            dataset.add_image(os.path.join(cropped_dir, f\"{filename}_{i+1}.png\"), label)\n",
    "\n",
    "            # Save the cropped image\n",
    "            cropped_im.save(os.path.join(cropped_dir, f\"{filename}_{i}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae833bf5-6915-40b5-9bff-1336f8f96607",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBTechGeeks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m300\u001b[39m}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# converting dictionary items to list\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dictlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# printing the first value of this list\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dictlist[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# Given Dictionary\n",
    "dictionary = {'this': 100, 'is': 200, 'BTechGeeks': 300}\n",
    "# converting dictionary items to list\n",
    "dictlist = list(dictionary.items())\n",
    "# printing the first value of this list\n",
    "print(dictlist[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166fa33-8347-4f5d-9123-ba6d28c58722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci_510_final] *",
   "language": "python",
   "name": "conda-env-dsci_510_final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
